<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Peng Tu</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/yash_logo.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Peng Tu</name>
              </p>
	      <p>I am a Senior Vision Algorithm Expert at RuqiMobility, which is a Smart Mobility Platform under GAC Group. My work primarily involves the intersection of Computer Vision and Efficient Deep Learning.
	      </p>
	      <p>
	      I receieved my master's degree from the <a href="https://www.ucas.ac.cn/">University of Chinese Academy of Sciences</a>.
	      My master's years was spent in the <a href="http://www.ict.ac.cn/">Institute of Computing Technology</a> and 
		 <a href="https://www.siat.ac.cn/index_64245.html">Shenzhen Institutes of Advanced Technology</a> of Chinese Academy of Sciences, under
		 the supervision of Prof. <a href="https://scholar.google.com/citations?user=XqdpqNcAAAAJ&hl=en">Jianbin Jiao</a>, 
		 Prof. <a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=zh-CN">Yu Qiao</a> and Prof. <a href="https://scholar.google.com.hk/citations?user=CVg2ja8AAAAJ&hl=zh-CN">Feng Dai</a>.
	      </p>
	      <p>Email: yh.peng.tu@gmail.com
	      </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/pengtu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/pengtu.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <p>
	      <ul>
		  <li><span style="color:red;font-weight: bold;">[New! 7/2023]</span> A new work (accepted by ICCV2023) building detector by considering energy cost -- <a href="https://arxiv.org/pdf/2301.06719.pdf">FemtoDet</a>, which enables extremely light-weight detectors that strike a balance between energy consumption and performance! &#128522;</li>
		  <li><span style="color:red;font-weight: bold;">[New! 4/2023]</span> Are you tired of dealing with offensive false positives in object detection? Look no further than our latest research -- Triplet ROIAlignment! &#128522;</li>
		  <li><span style="color:red;font-weight: bold;">[New! 11/2021]</span> <a href="https://arxiv.org/abs/2106.15064">GuidedMix-Net</a> was accepted in AAAI 2022! &#128522;</li>
	      </ul>
              </p>
            </td>
          </tr>
	</tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Services</heading>
              <p>
	      <ul>
		 Program Committee of AAAI'23/24/25, CVPR'24; Reviewer of IJCV'24.
	      </ul>
              </p>
            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p> <ul>*Equal Contribution; â€ Corresponding Author</ul> </p>
	    </td>
	  </tr>
	</table>

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="dirnet_stop()" onmouseover="dirnet_start()">
            <td style="padding:20px;width:33%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dirnet_image'>
		<img src='images/TRIAL_vis.png' width=240></div>
		<img src='images/TRIAL_vis.png' width=240></div>
              <script type="text/javascript">
                function dirnet_start() {
                  document.getElementById('dirnet_image').style.opacity = "1";
                }

                function dirnet_stop() {
                  document.getElementById('dirnet_image').style.opacity = "0";
                }
                dirnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:67%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2301.06719"><papertitle>Triplet ROI Alignment: Reducing False Positives for Generic Object Detection
</papertitle></a>
              <br>
	      <strong>Peng Tu</strong>,
              <a>Yawen Huang</a>,
              <a>Huimin Huang</a>,
	      <a href="https://scholar.google.com/citations?user=vAIECxgAAAAJ&hl=zh-CN">Yefeng Zheng</a>            
	      <br>
        <em>arxiv, in processing</em>, 2023
              <br>
              <p>
		Region-based deep neural networks have shown outstanding performances for object detection. 
		These detectors typically consist of separate branches for classification and localization with shared features and multi-task loss functions, resulting in cumulative errors and thus affecting detection power. 
		In this paper, we present a novel approach, Triplet ROI Alignment (TRIAL), which addresses the issue of false positive boxes in object detection. False positives, as the key factor, usually bring unfavorable impact causing degraded performances. The proposed TRIAL deals with this problem by directly suppressing the feature activation in these regions with three key procedures: 1) re-matching predictions into positive and negative boxes; 2) constructing triplet embeddings; 3) comparing positive and negative pairs in each triplet to guide detectors in generating more accurate and discriminative local visual features. 
		TRIAL is user-friendly and can be integrated with any efficient and two-stage detectors, including YOLOX, YOLOF, and FasterRCNN. 
		Extensive experiments demonstrate that TRIAL can significantly improve the mean average precision (mAP) of detectors by approximately 4$\%$ on the challenging COCO benchmark with reduced false positive samples. 
	      </p>
            </td>
          </tr>
		
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr onmouseout="dirnet_stop()" onmouseover="dirnet_start()">
            <td style="padding:20px;width:33%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dirnet_image'>
		<img src='images/TDT_vis.png' width=240></div>
		<img src='images/TDT_vis.png' width=240></div>
              <script type="text/javascript">
                function dirnet_start() {
                  document.getElementById('dirnet_image').style.opacity = "1";
                }

                function dirnet_stop() {
                  document.getElementById('dirnet_image').style.opacity = "0";
                }
                dirnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:67%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2311.18605"><papertitle>Learning Triangular Distribution in Visual World
</papertitle></a>
              <br>
	      <a>Ping Chen</a>,
	      <a>Xingpeng Zhang</a>,
	      <a>Chengtao Zhou</a>,
              <a>Dichao Fan</a>,
	      <strong>Peng Tu</strong>,
              <a>Le Zhang</a>,
              <a>Yanlin Qian</a>              
	      <br>
        <em>arxiv, in processing</em>, 2023
              <br>
              <p>
		Convolution neural network is successful in pervasive vision tasks, including label distribution learning, which usually takes the form of learning an injection from the nonlinear visual features to the well-defined labels. 
		However, how the discrepancy between features is mapped to the label discrepancy is ambient, and its correctness is not guaranteed.
		To address these problems, we study the mathematical connection between feature and its label, presenting a general and simple framework for label distribution learning.
		We propose a so-called Triangular Distribution Transform (TDT) to build an injective function between feature and label, guaranteeing that any symmetric feature discrepancy linearly reflects the difference between labels. 
		The proposed TDT can be used as a plug-in in mainstream backbone networks to address different label distribution learning tasks.
		Experiments on Facial Age Recognition, Illumination Chromaticity Estimation, and Aesthetics assessment show that TDT achieves on-par or better results than the prior arts.
	      </p>
            </td>
          </tr>
		
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="dirnet_stop()" onmouseover="dirnet_start()">
            <td style="padding:20px;width:33%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dirnet_image'>
		<img src='images/FemtoDet_vis.png' width=240></div>
		<img src='images/FemtoDet_vis.png' width=240></div>
              <script type="text/javascript">
                function dirnet_start() {
                  document.getElementById('dirnet_image').style.opacity = "1";
                }

                function dirnet_stop() {
                  document.getElementById('dirnet_image').style.opacity = "0";
                }
                dirnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:67%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2301.06719"><papertitle>Femtodet: an object detection baseline for energy versus performance tradeoffs
</papertitle></a>
              <br>
	      <strong>Peng Tu</strong>,
              <a>Xu Xie</a>,
              <a>Guo AI</a>,
              <a>Yuexiang Li</a>,
              <a>Yawen Huang</a>,
	      <a href="https://scholar.google.com/citations?user=vAIECxgAAAAJ&hl=zh-CN">Yefeng Zheng</a>
              <br>
        <em>International Conference on Computer Vision (ICCV)</em>, 2023
              <br>
              <p>Efficient detectors for edge devices are often optimized for parameters or speed count metrics, which remain in weak correlation with the energy of detectors. However, some vision applications of convolutional neural networks, such as always-on surveillance cameras, are critical for energy constraints. This paper aims to serve as a baseline by designing detectors to reach tradeoffs between energy and performance from two perspectives: 1) We extensively analyze various CNNs to identify low-energy architectures, including selecting activation functions, convolutions operators, and feature fusion structures on necks. These underappreciated details in past work seriously affect the energy consumption of detectors; 2) To break through the dilemmatic energy-performance problem, we propose a balanced detector driven by energy using discovered low-energy components named FemtoDet. In addition to the novel construction, we improve FemtoDet by considering convolutions and training strategy optimizations. Specifically, we develop a new instance boundary enhancement (IBE) module for convolution optimization to overcome the contradiction between the limited capacity of CNNs and detection tasks in diverse spatial representations, and propose a recursive warm-restart (RecWR) for optimizing training strategy to escape the sub-optimization of light-weight detectors by considering the data shift produced in popular augmentations. As a result, FemtoDet with only 68.77k parameters achieves a competitive score of 46.3 AP50 on PASCAL VOC and 1.11 W & 64.47 FPS on Qualcomm Snapdragon 865 CPU platforms. Extensive experiments on COCO and TJUDHD datasets indicate that the proposed method achieves competitive results in diverse scenes</p>
            </td>
          </tr>
		
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="dirnet_stop()" onmouseover="dirnet_start()">
            <td style="padding:20px;width:33%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dirnet_image'>
                  <img src='images/GuidedMix-Net2.jpg' width=240></div>
		<img src='images/GuidedMix-Net.jpg' width=240>
	      </div>
              <script type="text/javascript">
                function dirnet_start() {
                  document.getElementById('dirnet_image').style.opacity = "1";
                }

                function dirnet_stop() {
                  document.getElementById('dirnet_image').style.opacity = "0";
                }
                dirnet_stop()
              </script>
            </td>
            <td style="padding:20px;width:67%;vertical-align:middle">
                    <a href="https://arxiv.org/abs/2112.14015"><papertitle>GuidedMix-Net: Semi-supervised Semantic Segmentation by Using Labeled Images as Reference
</papertitle></a>
              <br>
	      <strong>Peng Tu</strong>,
              <a>Yawen Huang</a>,
              <a href="https://scholar.google.co.uk/citations?user=PcmyXHMAAAAJ&hl=en">Feng Zheng</a>,
              <a href="https://scholar.google.com.hk/citations?user=cv8_7usAAAAJ&hl=zh-CN">Zhenyu He</a>,
              <a href="https://scholar.google.com/citations?user=iYEcVaAAAAAJ&hl=zh-CN">Liujun Cao</a>,
	      <a href="https://scholar.google.com/citations?user=z84rLjoAAAAJ&hl=zh-CN">Ling Shao</a>
              <br>
        <em>Association for the Advancement of Artificial Intelligence (AAAI)</em>, 2022
              <br>
              <p>Semi-supervised learning is a challenging problem which aims to construct a model by learning from limited labeled examples. Numerous methods for this by treating labeled and unlabeled data separately often lead to discarding mass prior knowledge learned from the labeled examples. We propose a semi-supervised semantic segmentation method named GudedMix-Net,  which leverages labeled information to guide the learning of unlabeled instances.  Extensive experiments show that GuidedMix-Net significantly improves the mIoU by +7% compared to previous approaches.</p>
            </td>
          </tr>
			
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
	      Website template borrowed from <a href="https://jonbarron.info">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
